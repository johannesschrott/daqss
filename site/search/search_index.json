{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DaQSS","text":"<p>Many data-driven tasks require high-quality data to produce meaningful results. While data quality information can be computed on an ad-hoc basis when needed, a storage system is beneficial for analysis and for the providing of provenance information on data quality results. The idea of a data quality storage system is not new. However, existing developments are based on different types of data models but not solely on the relational data model, which is the most popular. As consequence, this work contributes a new Data Quality Storage System, called DaQSS, which is solely based on the relational data model. The new system allows to store values resulting from data quality measurements and aggregations and their associated metadata. Thus, the system provides provenance information on how data quality results have been computed. In addition to the concept, a proof-of-concept implementation is provided to demonstrate the practical applicability of the system.</p> <p>DaQSS consists of two parts: a database implementation, which is realized using the database management system PostgreSQL, and a Python package, which enables an easy integration of scripts with the database.</p> <p>This documentation is split into three parts:</p> <ul> <li>The page \"Set up DaQSS\" describes the steps that need to be taken in order to install and use DaQSS.</li> <li>The section \"Demonstration\" contains three examples of how DaQSS can be used.</li> <li>The section \"Technical Documentation\" provides background information on both   parts of DaQSS, the database implementation and the Python package.</li> </ul>"},{"location":"demo/","title":"Description","text":"<p>The demonstration of DaQSS is split into three parts. By doing so, the reuse and provenance aspect of the system can be highlighted more easily.</p> <ul> <li> <p>The first demonstration covers the computation of DQ result values through a DQ metric.   For provenance purposes, the metric and the result values are stored in DaQSS.</p> </li> <li> <p>The second demonstration shows a constraint-based aggregation of DQ result values, the storing   of the results and the metadata of the aggregation process in DaQSS.</p> </li> <li> <p>The third demonstration highlights how information that is stored in DaQSS, such as DQ metrics,   constraint-base aggregation processes, or DQ result values, can be reused.</p> </li> </ul>"},{"location":"demo/#datasets-used-in-the-demonstrations","title":"Datasets used in the Demonstrations","text":"<p>The demonstrations rely on two different data sets:</p> <ul> <li>A CSV file containing fake customer data with missing values   is used in the first and the third demonstration.</li> <li>A SQLite database created from three files containing information about products on   Amazon<sup>1</sup><sup>2</sup><sup>3</sup> is used in the second demonstration. The database is available for   download here.   The original files<sup>1</sup><sup>2</sup><sup>3</sup> are made available under the ODC Attribution License.</li> </ul> <p>To generate the SQLite database, the Python package <code>csv-to-sqlite</code> has been used in the following Python script:</p> <pre><code>import csv_to_sqlite\n\noptions = csv_to_sqlite.CsvOptions(typing_style=\"full\", encoding=\"utf-8\")\ninput_files = [\"amazon_us_products.csv\",\n               \"amazon_us_categories.csv\",\n               \"amz_ca_total_products_data_processed.csv\",\n               \"amz_in_total_products_data_processed.csv\"]\ncsv_to_sqlite.write_csv(input_files, \"product_information.sqlite\", options)\n</code></pre> <p>Following this, all records with a <code>listPrice</code> of <code>0</code> have been deleted from all tables of the databases tables and with a probability of 20%, the values of the column <code>price</code> and <code>listPrice</code> of a record have been swapped in order to pollute the database.</p> <ol> <li> <p>https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products, last visited on 16-Nov-2024\u00a0\u21a9\u21a9</p> </li> <li> <p>https://www.kaggle.com/datasets/asaniczka/amazon-canada-products-2023-2-1m-products, last visited on 16-Nov-2024\u00a0\u21a9\u21a9</p> </li> <li> <p>https://www.kaggle.com/datasets/asaniczka/amazon-india-products-2023-1-5m-products, last visited on 16-Nov-2024\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"demo1/","title":"Demo 1: Computation and Storing of DQ Measurement Results","text":"<p>This first demonstration shows how DQ measurement results and the metric used for their computation can be stored using DaQSS. As example in this demonstration, the completeness per record of a CSV file of  fake customer data with missing values is used.</p> <ol> <li>Import required Python packages:</li> </ol> In\u00a0[10]: Copied! <pre>from daqss import *  # for accessing the database part of DaQSS\n\nimport pandas\n# for the representation of data and the computation of DQ measurement results \n</pre> from daqss import *  # for accessing the database part of DaQSS  import pandas # for the representation of data and the computation of DQ measurement results  <ol> <li>Load data from a CSV file into a Pandas DataFrame:</li> </ol> In\u00a0[11]: Copied! <pre>data: pandas.DataFrame = pandas.read_csv(\"../demo_data/fake_customer_data.csv\")\ndata = data.set_index(\"CustomerID\", drop=False)\n# Create a DataFrame holding the data from the CSV file, set its index, \n# and retain its index as part of the data.\n\ndata_global_identifier: str = \"https://johannes.schrott.onl/fake_customer_data/fake_customer_data.csv\"\ndata_local_identifier: str = \"fake_customer_data.csv\"\n# Identifier that uniquely identifies the CSV file within DaQSS.\n# The global_identifier may have use other protocols and schemes than used above.\n\n# Show some records of the data:\ndata.head(5)\n</pre> data: pandas.DataFrame = pandas.read_csv(\"../demo_data/fake_customer_data.csv\") data = data.set_index(\"CustomerID\", drop=False) # Create a DataFrame holding the data from the CSV file, set its index,  # and retain its index as part of the data.  data_global_identifier: str = \"https://johannes.schrott.onl/fake_customer_data/fake_customer_data.csv\" data_local_identifier: str = \"fake_customer_data.csv\" # Identifier that uniquely identifies the CSV file within DaQSS. # The global_identifier may have use other protocols and schemes than used above.  # Show some records of the data: data.head(5) Out[11]: CustomerID FirstName LastName AddressID EmailAddress Phone Mobile CustomerID 77700.0 77700.0 Susan Ellis 64811.0 yorkvanessa@example.org 001-301-718-7221x9375 NaN 86928.0 86928.0 Caroline Barr 7716.0 NaN (363)436-7243x6386 NaN 16019.0 16019.0 Katherine Hess 40569.0 NaN 636.605.6222x71499 NaN 30163.0 30163.0 Gina Gross 33463.0 gluna@example.org NaN (971)505-3898x2674 85773.0 85773.0 Albert Hall 66320.0 john03@example.net 5863061645 001-377-640-5676x2420 <ol> <li>Define a DQ metric:</li> </ol> In\u00a0[12]: Copied! <pre>def arith_mean_completeness_per_row(row) -&gt; int:\n    \"\"\"Computes the unweighted arithmetic mean of the availability of values in a row.\n     The value 1 means a row is completely available, where 0 declares that there is no value in the row.\"\"\"\n    return 1 - row.isna().mean()\n</pre> def arith_mean_completeness_per_row(row) -&gt; int:     \"\"\"Computes the unweighted arithmetic mean of the availability of values in a row.      The value 1 means a row is completely available, where 0 declares that there is no value in the row.\"\"\"     return 1 - row.isna().mean()  <ol> <li>Apply the DQ metric to the data:</li> </ol> In\u00a0[13]: Copied! <pre>dq_measurement_results: pandas.Series = data.apply(arith_mean_completeness_per_row, axis=1)  # apply the DQ metric\n\n# Show some records of the DQ measurement results:\ndq_measurement_results.head(5)\n</pre> dq_measurement_results: pandas.Series = data.apply(arith_mean_completeness_per_row, axis=1)  # apply the DQ metric  # Show some records of the DQ measurement results: dq_measurement_results.head(5) Out[13]: <pre>CustomerID\n77700.0    0.857143\n86928.0    0.714286\n16019.0    0.714286\n30163.0    0.857143\n85773.0    1.000000\ndtype: float64</pre> <ol> <li>Store the DQ metric and the computed results into the database of DaQSS:</li> </ol> In\u00a0[14]: Copied! <pre>d = DaQSS()  # create a new instance of the DaQSS class to provide easy access to the database\n</pre> d = DaQSS()  # create a new instance of the DaQSS class to provide easy access to the database In\u00a0[16]: Copied! <pre># Create a new DQ dimension for the completeness metric,\n# in case it has not been created already.\n# If it already exists, nothing happens.\nd.store_dq_dimension(\"Completeness\")\n\n# Store the metric. If it already exists nothing happens. \n# If it is associated to a DQ dimension that does not exist,\n# the metric will still be created, but that dimension association is skipped.\nd.store_dq_metric(arith_mean_completeness_per_row, [\"Completeness\"], ROW)\n</pre> # Create a new DQ dimension for the completeness metric, # in case it has not been created already. # If it already exists, nothing happens. d.store_dq_dimension(\"Completeness\")  # Store the metric. If it already exists nothing happens.  # If it is associated to a DQ dimension that does not exist, # the metric will still be created, but that dimension association is skipped. d.store_dq_metric(arith_mean_completeness_per_row, [\"Completeness\"], ROW) <pre>WARNING:root:A DQ dimension with the name \"Completeness\" cannot be created, since a dimension with the same name already exists.\nWARNING:root:A DQ metric with the name \"arith_mean_completeness_per_row\" cannot be stored, since a metric with the same name already exists.\nWARNING:root:The DQ metric with the name \"arith_mean_completeness_per_row\" cannot be associated with the DQ dimension \"Completeness\", since\n - this association is already in place, or \n the dimension \"Completeness\" does not exist.\n</pre> In\u00a0[17]: Copied! <pre># DaQSS requires that all parent data elements of the data elements, of which the DQ is measured,\n# need to be also represented in the system\nd.store_data_element(data_global_identifier, data_local_identifier, TABLE)\n</pre> # DaQSS requires that all parent data elements of the data elements, of which the DQ is measured, # need to be also represented in the system d.store_data_element(data_global_identifier, data_local_identifier, TABLE) In\u00a0[18]: Copied! <pre># Store th DQ measurement result computed for each row\nd.store_dq_measurement_results_from_series(arith_mean_completeness_per_row, ROW, data_global_identifier,\n                                           dq_measurement_results)\n</pre> # Store th DQ measurement result computed for each row d.store_dq_measurement_results_from_series(arith_mean_completeness_per_row, ROW, data_global_identifier,                                            dq_measurement_results) <pre>WARNING:root:At least one DQ measurement result cannot be stored, since for at least one result value\n - no data element global_identifier was provided, or\n - the DQ metric computed two results for the same data element, or\n - the provided parent data element is not represented in DaQSS\n</pre> <p>The warning raised when storing DQ result values from <code>fake_customer_data.csv</code> is acceptable, since the CSV file contains rows that miss an identifier.</p>"},{"location":"demo1/#demo-1-computation-and-storing-of-dq-measurement-results","title":"Demo 1: Computation and Storing of DQ Measurement Results\u00b6","text":""},{"location":"demo2/","title":"Demo 2: Constraint-based Aggregation and Storing of Results and Metadata","text":"<p>This second demonstration shows how constraint-based aggregations can be performed and including the storing of their results in DaQSS. As example in this demonstration, the database containing product information, as described in the introduction of the demonstrations, is used.</p> <ol> <li>Import required Python packages:</li> </ol> In\u00a0[10]: Copied! <pre>import os\n\nos.environ[\"TQDM_DISABLE\"] = \"1\"  # Disable all progress bars, to retain a clear output\n\nfrom cobadq import *  # import the Python package for the constraint-based aggregation\nfrom daqss import *  # for accessing the database part of DaQSS\n\nimport pandas\n# for the representation of data and the computation of DQ measurement results \n</pre> import os  os.environ[\"TQDM_DISABLE\"] = \"1\"  # Disable all progress bars, to retain a clear output  from cobadq import *  # import the Python package for the constraint-based aggregation from daqss import *  # for accessing the database part of DaQSS  import pandas # for the representation of data and the computation of DQ measurement results  <ol> <li>Connect to the database tables and create their representations in DaQSS:</li> </ol> In\u00a0[11]: Copied! <pre>db_connection: str = \"sqlite:///../demo_data/product_information.sqlite\"\n\ncanada = DataSource.from_dataframe(pandas.read_sql_table(\"ca_products\", db_connection, index_col=\"asin\").head(100),\n                                   \"ca_products\")\nindia = DataSource.from_dataframe(pandas.read_sql_table(\"in_products\", db_connection, index_col=\"asin\").head(100),\n                                  \"in_products\")\nusa = DataSource.from_dataframe(pandas.read_sql_table(\"us_products\", db_connection, index_col=\"asin\").head(100),\n                                \"us_products\")\n</pre> db_connection: str = \"sqlite:///../demo_data/product_information.sqlite\"  canada = DataSource.from_dataframe(pandas.read_sql_table(\"ca_products\", db_connection, index_col=\"asin\").head(100),                                    \"ca_products\") india = DataSource.from_dataframe(pandas.read_sql_table(\"in_products\", db_connection, index_col=\"asin\").head(100),                                   \"in_products\") usa = DataSource.from_dataframe(pandas.read_sql_table(\"us_products\", db_connection, index_col=\"asin\").head(100),                                 \"us_products\")  In\u00a0[12]: Copied! <pre>d = DaQSS()\ndb = Database()\ndb.identifier = db_connection\ndb.values = [canada, india, usa]\n\nd.store_data_element(db.identifier, db.identifier, DATABASE)\nfor table in db.values:\n    d.store_data_element(db.identifier + \"/\" + table.identifier, table.identifier, TABLE, db.identifier)\n</pre> d = DaQSS() db = Database() db.identifier = db_connection db.values = [canada, india, usa]  d.store_data_element(db.identifier, db.identifier, DATABASE) for table in db.values:     d.store_data_element(db.identifier + \"/\" + table.identifier, table.identifier, TABLE, db.identifier)  <pre>WARNING:root:The data element with the global_identifier \"sqlite:///../demo_data/product_information.sqlite\" cannot be stored, since an data element with the same global_identifier already exists.\nWARNING:root:The data element with the global_identifier \"sqlite:///../demo_data/product_information.sqlite/ca_products\" cannot be stored, since \n - an data element with the same global_identifier already exists, or\n - the provided parent data element global_identifier \"sqlite:///../demo_data/product_information.sqlite does not exist.\nWARNING:root:The data element with the global_identifier \"sqlite:///../demo_data/product_information.sqlite/in_products\" cannot be stored, since \n - an data element with the same global_identifier already exists, or\n - the provided parent data element global_identifier \"sqlite:///../demo_data/product_information.sqlite does not exist.\nWARNING:root:The data element with the global_identifier \"sqlite:///../demo_data/product_information.sqlite/us_products\" cannot be stored, since \n - an data element with the same global_identifier already exists, or\n - the provided parent data element global_identifier \"sqlite:///../demo_data/product_information.sqlite does not exist.\n</pre> <ol> <li>Define and perform the constraint-based aggregation:</li> </ol> In\u00a0[14]: Copied! <pre># Define constraints\nprice_is_greater_than_listprice_name = \"price is greater than list price\"\nprice_is_greater_than_listprice_con = \"price &gt; listPrice\"\nd.store_aggregation_constraint(price_is_greater_than_listprice_name,\n                               \"Is fulfilled if the value in a column \\\"price\\\" \" +\n                               \"is greater than the value in a column \\\"listPrice\\\".\",\n                               price_is_greater_than_listprice_con, ROW)\nelse_con = \"else\"\nd.store_aggregation_constraint(else_con, \"Is fulfilled is no other constraint is fulfilled.\", else_con, ROW)\n\n# Define aggregation functions\nvalue_zero = \"0\"\nvalue_zero_name = \"Row is zero\"\nd.store_aggregation_function(\"Row is zero\", \"The aggregated value for a row will be zero\", value_zero, VALUE, ROW)\nvalue_one = \"1\"\nvalue_one_name = \"Row is one\"\nd.store_aggregation_function(\"Row is one\", \"The aggregated value for a row will be one\", value_one, VALUE, ROW)\n\nd.store_dq_dimension(\"Correctness\")\n\nagg_process_name = \"Product price correctness\"\nd.store_aggregation_process(agg_process_name,\n                            \"Check correctness of product prices in per row in the whole database.\",\n                            f\"SELECT value_level.* FROM data_element as value_level, data_element as row_level, \" +\n                            \"data_element as table_level \" +\n                            \"WHERE value_level.parent_data_element_global_identifier = row_level.data_element_global_identifier \" +\n                            \"AND row_level.parent_data_element_global_identifier = table_level.data_element_global_identifier \" +\n                            \"AND table_level.parent_data_element_global_identifier = '{db_connection}'\",\n                            [(price_is_greater_than_listprice_name, value_one_name), (else_con, value_zero_name)],\n                            [\"Correctness\"])\n</pre> # Define constraints price_is_greater_than_listprice_name = \"price is greater than list price\" price_is_greater_than_listprice_con = \"price &gt; listPrice\" d.store_aggregation_constraint(price_is_greater_than_listprice_name,                                \"Is fulfilled if the value in a column \\\"price\\\" \" +                                \"is greater than the value in a column \\\"listPrice\\\".\",                                price_is_greater_than_listprice_con, ROW) else_con = \"else\" d.store_aggregation_constraint(else_con, \"Is fulfilled is no other constraint is fulfilled.\", else_con, ROW)  # Define aggregation functions value_zero = \"0\" value_zero_name = \"Row is zero\" d.store_aggregation_function(\"Row is zero\", \"The aggregated value for a row will be zero\", value_zero, VALUE, ROW) value_one = \"1\" value_one_name = \"Row is one\" d.store_aggregation_function(\"Row is one\", \"The aggregated value for a row will be one\", value_one, VALUE, ROW)  d.store_dq_dimension(\"Correctness\")  agg_process_name = \"Product price correctness\" d.store_aggregation_process(agg_process_name,                             \"Check correctness of product prices in per row in the whole database.\",                             f\"SELECT value_level.* FROM data_element as value_level, data_element as row_level, \" +                             \"data_element as table_level \" +                             \"WHERE value_level.parent_data_element_global_identifier = row_level.data_element_global_identifier \" +                             \"AND row_level.parent_data_element_global_identifier = table_level.data_element_global_identifier \" +                             \"AND table_level.parent_data_element_global_identifier = '{db_connection}'\",                             [(price_is_greater_than_listprice_name, value_one_name), (else_con, value_zero_name)],                             [\"Correctness\"])  In\u00a0[15]: Copied! <pre>result_per_row: DataSource = aggregate(\n    db,\n    d.retrieve_aggregation_process_by_name_as_aggregation_specification(agg_process_name),\n    DataGranularity.ROW\n)\n</pre> result_per_row: DataSource = aggregate(     db,     d.retrieve_aggregation_process_by_name_as_aggregation_specification(agg_process_name),     DataGranularity.ROW ) <ol> <li>Store the aggregation results in DaQSS:</li> </ol> In\u00a0[16]: Copied! <pre>for table in result_per_row.values:\n    series = table.to_dataframe().iloc[:, 0]\n    d.store_dq_aggregation_results_from_series(\"Product price correctness\", ROW, db.identifier + \"/\" + table.identifier,\n                                               series)\n</pre> for table in result_per_row.values:     series = table.to_dataframe().iloc[:, 0]     d.store_dq_aggregation_results_from_series(\"Product price correctness\", ROW, db.identifier + \"/\" + table.identifier,                                                series) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demo2/#demo-2-constraint-based-aggregation-and-storing-of-results-and-metadata","title":"Demo 2: Constraint-based Aggregation and Storing of Results and Metadata\u00b6","text":""},{"location":"demo3/","title":"Demo 3: Use Information from DaQSS","text":"<p>Under the consideration of two possible use cases, this third demonstration shows how DQ results and their metadata stored in DaQSS can be utilized.</p> <ol> <li>Import required Python packages:</li> </ol> In\u00a0[6]: Copied! <pre>import os\n\nos.environ[\"TQDM_DISABLE\"] = \"1\"  # Disable all progress bars, to retain a clear output\n\nfrom cobadq import *  # import the Python package for the constraint-based aggregation\nfrom daqss import *  # for accessing the database part of DaQSS\nfrom sqlalchemy import text  # Used to directly query the database\n\nimport pandas\n# for the representation of data and the computation of DQ measurement results \n</pre> import os  os.environ[\"TQDM_DISABLE\"] = \"1\"  # Disable all progress bars, to retain a clear output  from cobadq import *  # import the Python package for the constraint-based aggregation from daqss import *  # for accessing the database part of DaQSS from sqlalchemy import text  # Used to directly query the database  import pandas # for the representation of data and the computation of DQ measurement results  In\u00a0[7]: Copied! <pre>d = DaQSS()  # Create a DaQSS object\n</pre> d = DaQSS()  # Create a DaQSS object <p>In this use case, the following question is answered:</p> <p>Which records of the fake customer data have an DQ measurement result value less than 0.5 for the metric \"arith_mean_completeness_per_row\" (cf. the first demonstration)?</p> <p>By directly using an SQLAlchemy Connection arbitrary queries can be executed on the database of DaQSS (cf.  the documentation of the schema).</p> In\u00a0[3]: Copied! <pre>fake_customer_global_identifier: str = \"https://johannes.schrott.onl/fake_customer_data/fake_customer_data.csv\"\nmetric_name = \"arith_mean_completeness_per_row\"\n\nwith d.connect() as connection:\n    result: list = list(connection.execute(\n        text(\"SELECT data_element_local_identifier, result_value FROM data_element, dq_result \" +\n             f\"WHERE dq_result.calculated_by_dq_metric = :metric \" +\n             \"AND dq_result.computed_on_data_element_global_id = data_element.data_element_global_identifier \" +\n             f\"AND data_element.parent_data_element_global_identifier = :id \" +\n             \"AND dq_result.result_value &lt; 0.5\"), {\"metric\": metric_name, \"id\": fake_customer_global_identifier}\n    )\n    )\n    result.sort(key=lambda row: row[1])\n    print(result[:5])  # For demonstration purpose, return the 5 smallest metric results\n</pre> fake_customer_global_identifier: str = \"https://johannes.schrott.onl/fake_customer_data/fake_customer_data.csv\" metric_name = \"arith_mean_completeness_per_row\"  with d.connect() as connection:     result: list = list(connection.execute(         text(\"SELECT data_element_local_identifier, result_value FROM data_element, dq_result \" +              f\"WHERE dq_result.calculated_by_dq_metric = :metric \" +              \"AND dq_result.computed_on_data_element_global_id = data_element.data_element_global_identifier \" +              f\"AND data_element.parent_data_element_global_identifier = :id \" +              \"AND dq_result.result_value &lt; 0.5\"), {\"metric\": metric_name, \"id\": fake_customer_global_identifier}     )     )     result.sort(key=lambda row: row[1])     print(result[:5])  # For demonstration purpose, return the 5 smallest metric results <pre>[('25221.0', Decimal('0.2857142857142857')), ('78828.0', Decimal('0.2857142857142857')), ('73946.0', Decimal('0.2857142857142857')), ('39076.0', Decimal('0.2857142857142857')), ('76087.0', Decimal('0.2857142857142857'))]\n</pre> In\u00a0[8]: Copied! <pre>metric_name = \"arith_mean_completeness_per_row\"\n\nmetric = d.retrieve_dq_metric_implementation_by_name(\"arith_mean_completeness_per_row\")\n\ndemo_dataframe = pandas.DataFrame({\"index\": [1], \"col1\": [None], \"col2\": [1]})\ndemo_dataframe = demo_dataframe.set_index(\"index\")\ndemo_dataframe.head()\n</pre> metric_name = \"arith_mean_completeness_per_row\"  metric = d.retrieve_dq_metric_implementation_by_name(\"arith_mean_completeness_per_row\")  demo_dataframe = pandas.DataFrame({\"index\": [1], \"col1\": [None], \"col2\": [1]}) demo_dataframe = demo_dataframe.set_index(\"index\") demo_dataframe.head() Out[8]: col1 col2 index 1 None 1 In\u00a0[9]: Copied! <pre>res = demo_dataframe.apply(metric, axis=1)\nres.head()\n</pre> res = demo_dataframe.apply(metric, axis=1) res.head() Out[9]: <pre>index\n1    0.5\ndtype: float64</pre>"},{"location":"demo3/#demo-3-use-information-from-daqss","title":"Demo 3: Use Information from DaQSS\u00b6","text":""},{"location":"demo3/#analysis-of-dq-results","title":"Analysis of DQ Results\u00b6","text":""},{"location":"demo3/#reuse-of-definitions","title":"Reuse of Definitions\u00b6","text":"<p>This use case shows through a small toy example, how DQ metrics can be reused:</p>"},{"location":"set-up/","title":"Set up DaQSS","text":"<p>DaQSS consists of two parts: (1) a database part and (2) a Python package called <code>daqss</code> that enables an integration into Python-based data processes. In the following, the setup process is described for each part.</p>"},{"location":"set-up/#database-part","title":"Database Part","text":"<p>There are two ways of how to set up the database part. Both ways have in common, that the files from https://johannes.schrott.onl/daqss/db_setup.zip are needed.</p> <p>On the one hand, it is possible to manually set up a PostgreSQL database using the SQL scripts provided in the <code>database_setup</code> directory of the project. On the other hand, a Docker Compose file that automatically configures, populates, and runs a PostgreSQL database in a container is provided.</p> <p>While the manual setup provides more potential for the configuration of the database, e.g., creating multiple users with different permission, using the Docker Compose file is the easiest and quickest way to setup a working installation of DaQSS. This way for setup provides only limited configuration options that can be set by using environment variables. The following table compares the system requirements and the setups steps that need to be taken for either way of database setup:</p> (1) Manual setup (2) Docker Compose Required software PostgreSQL 17 Docker Compose or Podman Compose Setup steps to be taken <ol><li>Installation and initial configuration of the PostgreSQL database</li><li>Create and populate the database and the tables required for DaQSS using the provided SQL scripts.</li><li>Create users, set fine-granular permissions etc.</li></ol> <ol><li>Define environment variables</li><li>Run <code>docker compose up</code> or <code>podman-compose up</code></li></ol>"},{"location":"set-up/#python-package","title":"Python Package","text":"<p>Although the database part of DaQSS alone could already be useful, its full potential can only be unleashed together with the Python package, which enables access to the database part of DaQSS directly from Python.</p> <p>In order to install the package, Python must be installed having a version of at least 3.12. Following this, the command <code>pip install git+https://github.com/johannesschrott/daqss.git</code> must be run to install the current version of the package together with its dependencies into the current Python environment. Before being able to use the installed Python pacakge, it must be ensured that the environment variables required for the Python package are set. This could be achieved, e.g., by placing an .env file in the working directory from which the Python package will be used.</p>"},{"location":"technical/api/","title":"Python API","text":"<p>This page describes all symbols that are available after importing the DaQSS package through <code>from daqss import *</code>.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.COLUMN","title":"<code>COLUMN = LevelOfDataGranularity('column', 1)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity. Covers column of data values, i.e. one attribute of data values.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.DATABASE","title":"<code>DATABASE = LevelOfDataGranularity('database', 3)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity. Covers all data values contained in a database.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.LevelOfDataGranularity","title":"<code>LevelOfDataGranularity = namedtuple('LevelOfDataGranularity', ['name', 'ordering'])</code>  <code>module-attribute</code>","text":"<p>A named tuple  that contains the <code>name</code> and the <code>ordering</code> of a level of data granularity.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.ROW","title":"<code>ROW = LevelOfDataGranularity('row', 1)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity. Covers a row (a record) of data values.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.SYSTEM","title":"<code>SYSTEM = LevelOfDataGranularity('system', 4)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity.  Covers all data values contained in a system, e.g., a system that integrates multiple databases.</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.TABLE","title":"<code>TABLE = LevelOfDataGranularity('table', 2)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity. Covers all data values contained in a table (a relation).</p>"},{"location":"technical/api/#src.daqss.levels_of_data_granularity.VALUE","title":"<code>VALUE = LevelOfDataGranularity('value', 0)</code>  <code>module-attribute</code>","text":"<p>An instance of the named tuple LevelOfDataGranularity. Covers a single data value.</p>"},{"location":"technical/api/#src.daqss.api.DQResult","title":"<code>DQResult = namedtuple('DQResult', ['name', 'ordering'])</code>  <code>module-attribute</code>","text":"<p>A named tuple  that contains the <code>name</code> and the <code>ordering</code> of a level of data granularity.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS","title":"<code>DaQSS</code>","text":"<p>Instances of the class DaQSS hold the connection to the PostgreSQL database and provide functions  that enable a convenient access to the system. As described for the initializer, the connection to the database is configured using environment variables.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new DaQSS instance that can connect to a DaQSS database. The connection settings must be customized using  environment variables:</p> <p>Mandatory environment variables:</p> <ul> <li><code>DAQSS_USERNAME</code></li> <li><code>DAQSS_PASSWORD</code></li> </ul> <p>Optional environment variables:</p> <ul> <li><code>DAQSS_HOST</code> - default value: <code>localhost:5432</code></li> <li><code>DAQSS_DATABASE</code> - default value: <code>daqss</code></li> </ul>"},{"location":"technical/api/#src.daqss.api.DaQSS.connect","title":"<code>connect()</code>","text":"<p>Returns the SQLAlchemy Connection to directly query the system using SQL.</p> <p>Returns:</p> Type Description <code>Connection</code> <p>The connection with DaQSS that can be used to query it using SQL.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.retrieve_aggregation_constraint_formula_by_name","title":"<code>retrieve_aggregation_constraint_formula_by_name(name)</code>","text":"<p>Retrieves the formula of an aggregation constraint suitable for usage with CobADQ by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the aggregation constraint to be retrieved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that contains the formula of an aggregation constraint for usage with CobADQ.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.retrieve_aggregation_function_expression_by_name","title":"<code>retrieve_aggregation_function_expression_by_name(name)</code>","text":"<p>Retrieves the expression of an aggregation function by its name suitable for usage with CobADQ.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the aggregation function to be retrieved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that contains the expression an aggregation function for usage with CobADQ.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.retrieve_aggregation_process_by_name_as_aggregation_specification","title":"<code>retrieve_aggregation_process_by_name_as_aggregation_specification(name)</code>","text":"<p>Retrieves the specification of an aggregation process by its name, directly to be used with CobADQ.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the aggregation process to be retrieved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that contains the specification of an aggregation directly for usage with CobADQ.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.retrieve_dq_metric_implementation_by_name","title":"<code>retrieve_dq_metric_implementation_by_name(name)</code>","text":"<p>Retrieves the implementation of a metric in the form of a Python callable by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric implementation to be retrieved.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The metric implementation in form of a callable.</p>"},{"location":"technical/api/#src.daqss.api.DaQSS.retrieve_levels_of_data_granularity","title":"<code>retrieve_levels_of_data_granularity()</code>","text":"<p>Connects to the DaQSS database and retrieves the levels of data granularity known to DaQSS in the form of a list that contains instances of the [named tuple][src.daqss.level_of_data_granularity.LevelOfDataGranularity].</p> <p>Returns:</p> Type Description <code>list[LevelOfDataGranularity]</code> <p>A list of the levels of data granularity containing their names and ordering in a</p> <code>list[LevelOfDataGranularity]</code> <p>[named tuple][src.daqss.level_of_data_granularity.LevelOfDataGranularity].                                Example result:                <code>[(\"value\", 0), (\"row\", 1)]</code></p>"},{"location":"technical/api/#src.daqss.api.DaQSS.store_aggregation_constraint","title":"<code>store_aggregation_constraint(name, description, constraint, level_of_data_granularity)</code>","text":"<p>Stores an aggregation constraint in the database part of DaQSS.</p> <p>If storing fails, a warning is logged.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name under which the aggregation constraint can be uniquely identified.</p> required <code>description</code> <code>str</code> <p>The description of what leads to fulfillment of the aggregation constraint.</p> required <code>constraint</code> <code>str</code> <p>The aggregation constraint encoded in CobADQs custom language</p> required <code>level_of_data_granularity</code> <code>LevelOfDataGranularity</code> <p>The level of data granularity at which the constraint can be applied.</p> required"},{"location":"technical/api/#src.daqss.api.DaQSS.store_aggregation_function","title":"<code>store_aggregation_function(name, description, expression, source_level_of_data_granularity, target_level_of_data_granularity)</code>","text":"<p>Stores an aggregation function in the database part of DaQSS.</p> <p>If storing fails, a warning is logged.</p> <pre><code>   Args:\n       name: A name under which the aggregation function can be uniquely identified.\n       description: The description of how the aggregation function works.\n       expression: The expression of the aggregation function encoded in CobADQs custom language.\n       source_level_of_data_granularity: The level of data granularity of the values to aggregate.\n       target_level_of_data_granularity: The level of data granularity of the aggregation result.\n</code></pre>"},{"location":"technical/api/#src.daqss.api.DaQSS.store_aggregation_process","title":"<code>store_aggregation_process(name, description, query_for_dq_results, constraints_and_functions, dimensions)</code>","text":"<p>Stores an aggregation process in the database part of DaQSS.</p> <p>If storing fails, a warning is logged.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name under which the aggregation process can be uniquely identified.</p> required <code>description</code> <code>str</code> <p>The description of the aggregation process works.</p> required <code>query_for_dq_results</code> <code>str</code> <p>The query that returns the values to aggregate.</p> required <code>dimensions</code> <code>list[str]</code> <p>A list of names of DQ dimensions to which the aggregation process is associated.</p> required <code>constraints_and_functions</code> <code>list[tuple[str, str]]</code> <p>A list of tuples that contain the names of the constraints and aggregation functions used within the aggregation process.</p> required"},{"location":"technical/api/#src.daqss.api.DaQSS.store_data_element","title":"<code>store_data_element(global_identifier, local_identifier, level_of_data_granularity, parent_identifier=None)</code>","text":"<p>Store the representation of a data element in the connected PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>global_identifier</code> <code>str</code> <p>The globally unique identifier of a data element to be represented in DaQSS.</p> required <code>local_identifier</code> <code>str</code> <p>The identifier that identifies a data element within its parent data element.</p> required <code>level_of_data_granularity</code> <code>LevelOfDataGranularity</code> <p>The level of data granularity of the data element.</p> required <code>parent_identifier</code> <code>str | None</code> <p>The unique identifier of a data element in which this data element is contained. Defaults to None, which means that the data element is not contained in any other data element.</p> <code>None</code>"},{"location":"technical/api/#src.daqss.api.DaQSS.store_dq_aggregation_results_from_series","title":"<code>store_dq_aggregation_results_from_series(aggregation_process, level_of_data_granularity, parent_data_element, values)</code>","text":"<p>Stores multiple result values computed by a DQ aggregation process. If they do not exist, the representations of the data of which the DQ was computed are created.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_process</code> <code>str</code> <p>The name of the aggregation process that was used to compute the result values.</p> required <code>level_of_data_granularity</code> <code>LevelOfDataGranularity</code> <p>The level of data granularity of the individual result values.</p> required <code>parent_data_element</code> <code>str</code> <p>The parent data element which contains the individual result values to be stored.</p> required <code>values</code> <code>Series</code> <p>A Pandas series of values that contains the computed result values and the local identifiers of their corresponding data values.</p> required"},{"location":"technical/api/#src.daqss.api.DaQSS.store_dq_dimension","title":"<code>store_dq_dimension(name, description=None, is_subdimension_of=None)</code>","text":"<p>Stores a DQ dimension in the database part of DaQSS.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the DQ dimension to be stored.</p> required <code>description</code> <code>str or None</code> <p>The description of the DQ dimension to be stored.</p> <code>None</code> <code>is_subdimension_of</code> <code>str or None</code> <p>The name of the dimension of which this dimension is a sub-dimension.</p> <code>None</code>"},{"location":"technical/api/#src.daqss.api.DaQSS.store_dq_measurement_results_from_series","title":"<code>store_dq_measurement_results_from_series(dq_metric, level_of_data_granularity, parent_data_element, values)</code>","text":"<p>Stores multiple result values computed by a DQ metric. If they do not exist, the representations of the data of which the DQ was computed are created.</p> <p>Parameters:</p> Name Type Description Default <code>dq_metric</code> <code>Callable</code> <p>The name of the DQ metric that was used to compute the result values.</p> required <code>level_of_data_granularity</code> <code>LevelOfDataGranularity</code> <p>The level of data granularity of the individual result values.</p> required <code>parent_data_element</code> <code>str</code> <p>The parent data element which contains the individual result values to be stored.</p> required <code>values</code> <code>Series</code> <p>A Pandas series of values that contains the computed result values and the local identifiers of their corresponding data values.</p> required"},{"location":"technical/api/#src.daqss.api.DaQSS.store_dq_metric","title":"<code>store_dq_metric(dq_metric, dimensions, level_of_data_granularity)</code>","text":"<p>Store the Python implementation of a DQ metric in the connected PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>dq_metric</code> <code>Callable</code> <p>The Python implementation of the DQ metric to be stored.</p> required <code>dimensions</code> <code>list[str]</code> <p>The DQ dimensions for which the DQ metric computes values.</p> required <code>level_of_data_granularity</code> <code>LevelOfDataGranularity</code> <p>The level of data granularity on which the DQ Metric operates.</p> required"},{"location":"technical/dependencies/","title":"Requirements and Dependencies","text":"<p>The Python Package of DaQSS requires Python, Version 3.12 or higher, and depends on the following Python packages that are automatically installed when installing DaQSS:</p> <ul> <li><code>dill</code>: Dill provides serialization functionalities. In DaQSS, it is used   to (de)serialize the Python implementation of DQ metrics.</li> <li><code>dotenv</code>: The dotenv package provides the functionality to use   environment variables defined in <code>.env</code> files.</li> <li><code>pandas</code>: The Pandas package provides data structures and functions for working with   tabular data. In DaQSS, DQ results to be stored and to be retrieved are contained in   a Pandas DataFrame.</li> <li><code>psycopg2</code>: Pyscopg is a database adapter that enables Python to connect to PostgreSQL   databases.</li> <li><code>sqlalchemy</code>: SQLAlchemy describes itself as \"the Python SQL toolkit and Object   Relational Mapper\". In DaQSS, classes from the package are used as an abstraction layer for connecting to, inserting   into, and querying a PostgreSQL database.</li> </ul> <p>For the generation of this documentation MkDocs is used. The dependencies required for the generation can be installed by running <code>pip install \"daqss[docs] @ git+https://github.com/johannesschrott/daqss.git\"</code>.</p>"},{"location":"technical/environment_variables/","title":"Source Code Documentation of the Environment Variables","text":"<p>For the configuration of the Docker compose installation variant of the database part of DaQSS and for the configuration of the connection from the Python Package to the database part, environment variables are used.</p>"},{"location":"technical/environment_variables/#description","title":"Description","text":"<p>DaQSS relies on four environment variables, of which two must be set in order to be able to use the system:</p> <ul> <li><code>DAQSS_DATABASE</code>: The name of the database that contains the tables that hold the data of DaQSS. This variable is only   used by the Python package and should only be set when a manual setup of the PostgreSQL database has been performed.   Defaults to <code>daqss</code> if not provided.</li> <li><code>DAQSS_HOST</code>: The database host that holds the data of DaQSS.   The value of this environment variable must include the address and port of the database server. This variable is only   used by the Python package and should only be set when a manual setup of the PostgreSQL database has been performed or   when the Docker Compose setup was performed on any other host than <code>localhost</code>. Defaults to <code>localhost</code> if not   provided.</li> </ul> Example values <p>E.g., <code>0.0.0.0:5432</code> or <code>myhost.com:5432</code>. </p> <ul> <li> <p><code>DAQSS_PASSWORD</code>:   The password used for connecting to the database that holds the data of DaQSS. This variable must always be   provided when using DaQSS. In case the Docker Compose database setup is used, the value of this variable is the   password that can be used to connect to the database.</p> </li> <li> <p><code>DAQSS_USERNAME</code>:   The username used for connecting to the database that holds the data of DaQSS. This variable must always be   provided when using DaQSS. In case the Docker Compose database setup is used, the value of this variable is the   username that can be used to connect to the database.</p> </li> </ul>"},{"location":"technical/environment_variables/#example","title":"Example","text":"<p>The environment files could be either set globally to be part of a terminal environment, e.g., by running commands like <code>export VARIABLE=value</code> on Linux, or locally by placing an <code>.env</code> file in the directory where DaQSS is used.</p> <p>An <code>.env</code> file can look like the one shown below:</p> .env<pre><code>DAQSS_USERNAME=\"johannes\"\nDAQSS_PASSWORD=\"mysupersecretpassword\"\n</code></pre>"},{"location":"technical/ev_code_reference/","title":"Code Reference","text":""},{"location":"technical/ev_code_reference/#src.daqss.environment_variables.EnvironmentVariables","title":"<code>EnvironmentVariables</code>","text":"<p>               Bases: <code>Enum</code></p> <p>For the configuration of DaQSS environment variables, as described in the following, are available. For a description of how the environment variables are used and for examples please see the dedicated documentation page.</p>"},{"location":"technical/ev_code_reference/#src.daqss.environment_variables.EnvironmentVariables.DAQSS_DATABASE","title":"<code>DAQSS_DATABASE = 'DAQSS_DATABASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the database that holds the data of DaQSS.</p>"},{"location":"technical/ev_code_reference/#src.daqss.environment_variables.EnvironmentVariables.DAQSS_HOST","title":"<code>DAQSS_HOST = 'DAQSS_HOST'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The database host used for connecting to the database that holds the data of DaQSS. The value of this environment variable must include the address and port of the database server.</p> Example <p>Example values for this environment variable would be <code>0.0.0.0:5432</code> or <code>myhost.com:5432</code>.</p>"},{"location":"technical/ev_code_reference/#src.daqss.environment_variables.EnvironmentVariables.DAQSS_PASSWORD","title":"<code>DAQSS_PASSWORD = 'DAQSS_PASSWORD'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The password used for connecting to the database that holds the data of DaQSS.</p>"},{"location":"technical/ev_code_reference/#src.daqss.environment_variables.EnvironmentVariables.DAQSS_USERNAME","title":"<code>DAQSS_USERNAME = 'DAQSS_USERNAME'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The username used for connecting to the database that holds the data of DaQSS.</p>"},{"location":"technical/postgresql_structure/","title":"PostgreSQL Database","text":"<p>The database part of DaQSS is realized using the DBMS PostgreSQL. PostgreSQL has been chosen since it is a popular, relational open-source DBMS (cf. the DB-Engines.com ranking). On this page, the conceptual model is introduced at first and as second a reference to the documentation of the relational database resulting from that model is provided.</p>"},{"location":"technical/postgresql_structure/#conceptual-model","title":"Conceptual Model","text":""},{"location":"technical/postgresql_structure/#database-implementation","title":"Database Implementation","text":"<p>The conceptual model is realized in PostgreSQL in the form of 11 tables. A documentation of the database implementation, which has been generated using the tool SchemaSpy, can be found under https://johannes.schrott.onl/daqss/database_docs.</p>"}]}